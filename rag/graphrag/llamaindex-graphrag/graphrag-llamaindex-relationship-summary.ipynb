{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7593fac-9444-45a7-9b9a-dfbe2c91eb5b",
   "metadata": {},
   "source": [
    "# Goal\n",
    "\n",
    "The goal of this project is to demonstrate how to build an end-to-end GraphRAG pipeline — starting from raw text processing, extracting structured knowledge using LLMs, detecting semantic communities within the knowledge graph, and ultimately enabling interpretable, context-aware question answering.\n",
    "By leveraging a combination of LLM-based extraction, graph algorithms, and semantic summarization, we transform unstructured text (Charles Dickens’ A Christmas Carol) into a structured, queryable knowledge base, offering a powerful and explainable alternative to traditional RAG systems.\n",
    "\n",
    "This project highlights how GraphRAG elevates information retrieval by:\n",
    "- Embedding semantic understanding at the chunk level,\n",
    "- Structuring knowledge into meaningful communities,\n",
    "- Summarizing communities with LLMs for human-readable insights,\n",
    "- Synthesizing multi-perspective answers grounded in structured knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37b3354-ba22-4276-a319-2bfd3008e21c",
   "metadata": {},
   "source": [
    "# 1. Setting Up LLM Access\n",
    "\n",
    "Instantiates a `gpt-3.5-turbo` powered language model for downstream text extraction and summarization tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cad627b6-59de-42e6-97fd-69dcd9d2d8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "import os\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = \"your-openai-api-key\"\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4014e76-e7e0-4b04-bcba-569eb93e45bc",
   "metadata": {},
   "source": [
    "# 2. GraphRAGExtractor\n",
    "\n",
    "A transformer component that:\n",
    "- Sends text chunks to an LLM guided by a structured prompt.\n",
    "- Extracts entities, types, and relationships.\n",
    "- Embeds semantic descriptions directly into metadata.\n",
    "- Outputs a triple structure into LlamaIndex’s graph format.\n",
    "\n",
    "This forms the knowledge foundation of the GraphRAG system.\n",
    "\n",
    "\n",
    "## 2.1. Environment Setup\n",
    "\n",
    "The script first ensures compatibility with asynchronous operations in environments like Jupyter notebooks by importing asyncio and applying nest_asyncio. This allows concurrent tasks, like parallel extraction from multiple text chunks, to run seamlessly.\n",
    "\n",
    "\n",
    "## 2.2. Class Overview\n",
    "\n",
    "The `GraphRAGExtractor` class inherits from `TransformComponent` and is initialized with the following:\n",
    "- An LLM instance `llm`, responsible for extracting knowledge from text.\n",
    "- A structured prompt `extract_prompt` that instructs the LLM to output entity-relation triplets.\n",
    "- A parsing function `parse_fn` that converts raw LLM responses into structured formats.\n",
    "- Parameters like `max_paths_per_chunk` (limits number of triplets) and `num_workers` (enables parallel processing).\n",
    "\n",
    "If a user does not explicitly supply a prompt or LLM, the extractor defaults to pre-configured values from LlamaIndex settings.\n",
    "\n",
    "\n",
    "## 2.3. Extraction Workflow\n",
    "\n",
    "The main extraction happens in two methods:\n",
    "- `_aextract()`:\n",
    "    - Takes a text node `BaseNode` as input.\n",
    "    - Sends the text to the LLM along with the extraction prompt.\n",
    "    - Parses the LLM output to obtain entities, entity descriptions, relationships, and relationship descriptions.\n",
    "    - Converts this information into EntityNode and Relation objects, which are stored inside the node’s metadata.\n",
    "- `acall()`:\n",
    "    - Manages asynchronous execution.\n",
    "    - Launches multiple` _aextract()` jobs in parallel across different nodes.\n",
    "    - Uses `run_jobs()` for efficient, scalable batch processing, especially for large datasets.\n",
    "\n",
    "Finally, the `__call__()` method simply wraps `acall()` into a synchronous call using `asyncio.run()`, making it easy for users to trigger extraction in a single step.\n",
    "\n",
    "\n",
    "The `GraphRAGExtractor` transforms unstructured documents into rich, structured graphs, embedding semantic understanding at the node level.\n",
    "These enriched nodes — carrying entity-relation metadata — serve as the foundation for building property graphs, detecting semantic communities, and enabling interpretable, context-rich retrieval in modern GenAI systems like GraphRAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b42d7ef1-005e-411e-bb8a-101bbce0a8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from typing import Any, List, Callable, Optional, Union, Dict\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from llama_index.core.async_utils import run_jobs\n",
    "from llama_index.core.indices.property_graph.utils import (\n",
    "    default_parse_triplets_fn,\n",
    ")\n",
    "from llama_index.core.graph_stores.types import (\n",
    "    EntityNode,\n",
    "    KG_NODES_KEY,\n",
    "    KG_RELATIONS_KEY,\n",
    "    Relation,\n",
    ")\n",
    "from llama_index.core.llms.llm import LLM\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core.prompts.default_prompts import (\n",
    "    DEFAULT_KG_TRIPLET_EXTRACT_PROMPT,\n",
    ")\n",
    "from llama_index.core.schema import TransformComponent, BaseNode\n",
    "from llama_index.core.bridge.pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class GraphRAGExtractor(TransformComponent):\n",
    "\n",
    "    llm: LLM\n",
    "    extract_prompt: PromptTemplate\n",
    "    parse_fn: Callable\n",
    "    num_workers: int\n",
    "    max_paths_per_chunk: int\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm: Optional[LLM] = None,\n",
    "        extract_prompt: Optional[Union[str, PromptTemplate]] = None,\n",
    "        parse_fn: Callable = default_parse_triplets_fn,\n",
    "        max_paths_per_chunk: int = 10,\n",
    "        num_workers: int = 4,\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "        from llama_index.core import Settings\n",
    "\n",
    "        if isinstance(extract_prompt, str):\n",
    "            extract_prompt = PromptTemplate(extract_prompt)\n",
    "\n",
    "        super().__init__(\n",
    "            llm=llm or Settings.llm,\n",
    "            extract_prompt=extract_prompt or DEFAULT_KG_TRIPLET_EXTRACT_PROMPT,\n",
    "            parse_fn=parse_fn,\n",
    "            num_workers=num_workers,\n",
    "            max_paths_per_chunk=max_paths_per_chunk,\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def class_name(cls) -> str:\n",
    "        return \"GraphExtractor\"\n",
    "\n",
    "    def __call__(\n",
    "        self, nodes: List[BaseNode], show_progress: bool = False, **kwargs: Any\n",
    "    ) -> List[BaseNode]:\n",
    "        \"\"\"Extract triples from nodes.\"\"\"\n",
    "        return asyncio.run(\n",
    "            self.acall(nodes, show_progress=show_progress, **kwargs)\n",
    "        )\n",
    "\n",
    "    async def _aextract(self, node: BaseNode) -> BaseNode:\n",
    "        \"\"\"Extract triples from a node.\"\"\"\n",
    "        assert hasattr(node, \"text\")\n",
    "\n",
    "        text = node.get_content(metadata_mode=\"llm\")\n",
    "        try:\n",
    "            llm_response = await self.llm.apredict(\n",
    "                self.extract_prompt,\n",
    "                text=text,\n",
    "                max_knowledge_triplets=self.max_paths_per_chunk,\n",
    "            )\n",
    "            entities, entities_relationship = self.parse_fn(llm_response)\n",
    "        except ValueError:\n",
    "            entities = []\n",
    "            entities_relationship = []\n",
    "\n",
    "        existing_nodes = node.metadata.pop(KG_NODES_KEY, [])\n",
    "        existing_relations = node.metadata.pop(KG_RELATIONS_KEY, [])\n",
    "        metadata = node.metadata.copy()\n",
    "        for entity, entity_type, description in entities:\n",
    "            metadata[\n",
    "                \"entity_description\"\n",
    "            ] = description  # Not used in the current implementation. But will be useful in future work.\n",
    "            entity_node = EntityNode(\n",
    "                name=entity, label=entity_type, properties=metadata\n",
    "            )\n",
    "            existing_nodes.append(entity_node)\n",
    "\n",
    "        metadata = node.metadata.copy()\n",
    "        for triple in entities_relationship:\n",
    "            subj, obj, rel, description = triple\n",
    "            subj_node = EntityNode(name=subj, properties=metadata)\n",
    "            obj_node = EntityNode(name=obj, properties=metadata)\n",
    "            metadata[\"relationship_description\"] = description\n",
    "            rel_node = Relation(\n",
    "                label=rel,\n",
    "                source_id=subj_node.id,\n",
    "                target_id=obj_node.id,\n",
    "                properties=metadata,\n",
    "            )\n",
    "\n",
    "            existing_nodes.extend([subj_node, obj_node])\n",
    "            existing_relations.append(rel_node)\n",
    "\n",
    "        node.metadata[KG_NODES_KEY] = existing_nodes\n",
    "        node.metadata[KG_RELATIONS_KEY] = existing_relations\n",
    "        return node\n",
    "\n",
    "    async def acall(\n",
    "        self, nodes: List[BaseNode], show_progress: bool = False, **kwargs: Any\n",
    "    ) -> List[BaseNode]:\n",
    "        \"\"\"Extract triples from nodes async.\"\"\"\n",
    "        jobs = []\n",
    "        for node in nodes:\n",
    "            jobs.append(self._aextract(node))\n",
    "\n",
    "        return await run_jobs(\n",
    "            jobs,\n",
    "            workers=self.num_workers,\n",
    "            show_progress=show_progress,\n",
    "            desc=\"Extracting paths from text\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6616755a-4066-4817-9598-87806746c81c",
   "metadata": {},
   "source": [
    "# 3. GraphRAGStore\n",
    "\n",
    "An intelligent graph store that:\n",
    " - Converts graphs to NetworkX format.\n",
    " - Applies graspologic’s Hierarchical Leiden for community detection.\n",
    " - Generates LLM-based summaries of intra-community relationships.\n",
    " - Stores and indexes these summaries for fast, structured query access.\n",
    "\n",
    "Essentially the provided code defines `GraphRAGStore`, a powerful extension of `SimplePropertyGraphStore`, designed to build communities within a knowledge graph and summarize their semantics using a large language model (LLM). This class serves as the critical middle layer in the GraphRAG architecture, where raw extracted knowledge is organized into meaningful clusters for efficient and interpretable retrieval.\n",
    "\n",
    "\n",
    "## 3.1. Class Purpose\n",
    "\n",
    "The `GraphRAGStore` is responsible for:\n",
    "- Structuring the property graph into semantic communities.\n",
    "- Summarizing each community’s internal relationships.\n",
    "- Providing a retrieval-ready format where each cluster represents a topic or tightly connected concept set.\n",
    "\n",
    "It introduces two configurable properties:\n",
    "- `community_summary`: A dictionary that holds generated summaries per community.\n",
    "- `max_cluster_size`: A parameter controlling the granularity of clusters during community detection.\n",
    "\n",
    "\n",
    "## 3.2. Generating Community Summaries\n",
    "\n",
    "The `generate_community_summary()` method sends a curated set of relationships to an LLM (via the ChatMessage interface) with explicit instructions:\n",
    "- It asks the LLM to read relationship patterns like entity1 → entity2 → relationship → description.\n",
    "- It instructs the LLM to synthesize a coherent, concise narrative capturing the essence of these relationships.\n",
    "- The output is cleaned to remove any AI system-specific prefixes (assistant:) before storage.\n",
    "\n",
    "This ensures that every semantic community has a human-readable, LLM-generated summary explaining its key ideas.\n",
    "\n",
    "\n",
    "## 3.3. Building Communities from the Graph\n",
    "\n",
    "The `build_communities()` method orchestrates the core clustering logic:\n",
    "- Converts the internal graph into a `NetworkX` graph format via `_create_nx_graph()`. Transforms the internal property graph into a NetworkX graph by creating nodes and edges, where edges carry relationship labels and descriptions.\n",
    "- Applies the Hierarchical Leiden algorithm `hierarchical_leiden` to detect densely connected groups of nodes (communities).\n",
    "- Extracts detailed information for each node based on their community using `_collect_community_info()`. For each node, it collects neighbor relationships only within the same community, ensuring that the community summary remains topically consistent.\n",
    "- Generates community summaries by calling `_summarize_communities()`. It converts the detailed intra-community relationships into a string and feeds it into the LLM to produce a single semantic summary per cluster.\n",
    "\n",
    "Thus, from a flat knowledge graph, the system creates high-level topical structures automatically.\n",
    "\n",
    "\n",
    "## 3.5. Lazy Summary Retrieval\n",
    "\n",
    "Finally, the `get_community_summaries()` method ensures that summaries are generated only when needed:\n",
    "- If summaries do not exist yet, it triggers a full build_communities() process.\n",
    "- Otherwise, it quickly retrieves the stored results.\n",
    "\n",
    "This lazy execution approach optimizes resource usage and avoids redundant computations during repeated querying.\n",
    "\n",
    "\n",
    "In the GraphRAG pipeline, `GraphRAGStore` plays a crucial role in structuring chaotic knowledge into thematic clusters, and distilling meaning from relationships — making the final retrieval system interpretable, scalable, and more aligned with how humans think (via structured summaries instead of isolated facts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e433bb7f-4e3f-441b-a079-ecc5677bfaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from llama_index.core.graph_stores import SimplePropertyGraphStore\n",
    "import networkx as nx\n",
    "from graspologic.partition import hierarchical_leiden\n",
    "\n",
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "\n",
    "class GraphRAGStore(SimplePropertyGraphStore):\n",
    "    community_summary = {}\n",
    "    max_cluster_size = 5\n",
    "\n",
    "    def generate_community_summary(self, text):\n",
    "        \"\"\"Generate summary for a given text using an LLM.\"\"\"\n",
    "        messages = [\n",
    "            ChatMessage(\n",
    "                role=\"system\",\n",
    "                content=(\n",
    "                    \"You are provided with a set of relationships from a knowledge graph, each represented as \"\n",
    "                    \"entity1->entity2->relation->relationship_description. Your task is to create a summary of these \"\n",
    "                    \"relationships. The summary should include the names of the entities involved and a concise synthesis \"\n",
    "                    \"of the relationship descriptions. The goal is to capture the most critical and relevant details that \"\n",
    "                    \"highlight the nature and significance of each relationship. Ensure that the summary is coherent and \"\n",
    "                    \"integrates the information in a way that emphasizes the key aspects of the relationships.\"\n",
    "                ),\n",
    "            ),\n",
    "            ChatMessage(role=\"user\", content=text),\n",
    "        ]\n",
    "        response = OpenAI().chat(messages)\n",
    "        clean_response = re.sub(r\"^assistant:\\s*\", \"\", str(response)).strip()\n",
    "        return clean_response\n",
    "\n",
    "    def build_communities(self):\n",
    "        \"\"\"Builds communities from the graph and summarizes them.\"\"\"\n",
    "        nx_graph = self._create_nx_graph()\n",
    "        community_hierarchical_clusters = hierarchical_leiden(\n",
    "            nx_graph, max_cluster_size=self.max_cluster_size\n",
    "        )\n",
    "        community_info = self._collect_community_info(\n",
    "            nx_graph, community_hierarchical_clusters\n",
    "        )\n",
    "        self._summarize_communities(community_info)\n",
    "\n",
    "    def _create_nx_graph(self):\n",
    "        \"\"\"Converts internal graph representation to NetworkX graph.\"\"\"\n",
    "        nx_graph = nx.Graph()\n",
    "        for node in self.graph.nodes.values():\n",
    "            nx_graph.add_node(str(node))\n",
    "        for relation in self.graph.relations.values():\n",
    "            nx_graph.add_edge(\n",
    "                relation.source_id,\n",
    "                relation.target_id,\n",
    "                relationship=relation.label,\n",
    "                description=relation.properties[\"relationship_description\"],\n",
    "            )\n",
    "        return nx_graph\n",
    "\n",
    "    def _collect_community_info(self, nx_graph, clusters):\n",
    "        \"\"\"Collect detailed information for each node based on their community.\"\"\"\n",
    "        community_mapping = {item.node: item.cluster for item in clusters}\n",
    "        community_info = {}\n",
    "        for item in clusters:\n",
    "            cluster_id = item.cluster\n",
    "            node = item.node\n",
    "            if cluster_id not in community_info:\n",
    "                community_info[cluster_id] = []\n",
    "\n",
    "            for neighbor in nx_graph.neighbors(node):\n",
    "                if community_mapping[neighbor] == cluster_id:\n",
    "                    edge_data = nx_graph.get_edge_data(node, neighbor)\n",
    "                    if edge_data:\n",
    "                        detail = f\"{node} -> {neighbor} -> {edge_data['relationship']} -> {edge_data['description']}\"\n",
    "                        community_info[cluster_id].append(detail)\n",
    "        return community_info\n",
    "\n",
    "    def _summarize_communities(self, community_info):\n",
    "        \"\"\"Generate and store summaries for each community.\"\"\"\n",
    "        for community_id, details in community_info.items():\n",
    "            details_text = (\n",
    "                \"\\n\".join(details) + \".\"\n",
    "            )  # Ensure it ends with a period\n",
    "            self.community_summary[\n",
    "                community_id\n",
    "            ] = self.generate_community_summary(details_text)\n",
    "\n",
    "    def get_community_summaries(self):\n",
    "        \"\"\"Returns the community summaries, building them if not already done.\"\"\"\n",
    "        if not self.community_summary:\n",
    "            self.build_communities()\n",
    "        return self.community_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e17888f-aeb6-4c2c-aff8-414c13afbac2",
   "metadata": {},
   "source": [
    "# 4. GraphRAGQueryEngine\n",
    "\n",
    "The provided code defines the `GraphRAGQueryEngine`, a custom query engine that sits at the final stage of the GraphRAG pipeline, enabling intelligent, structured retrieval based on community summaries rather than raw document chunks. This class plays a pivotal role in transforming fragmented information into coherent, high-quality answers tailored to user queries.\n",
    "\n",
    "\n",
    "## 4.1. Class Purpose\n",
    "\n",
    "The `GraphRAGQueryEngine` is designed to:\n",
    "- Retrieve community-level summaries from the structured graph `GraphRAGStore`.\n",
    "- Use a Large Language Model (LLM) to interpret each community in the context of the user’s query.\n",
    "- Aggregate multiple community-level answers into a single, unified response.\n",
    "\n",
    "In other words, it orchestrates the retrieval, interpretation, and synthesis of knowledge extracted and organized by earlier stages of the pipeline.\n",
    "\n",
    "\n",
    "## 4.2. Query Processing Workflow\n",
    "\n",
    "The main method, `custom_query(query_str)`, governs the full query execution flow:\n",
    "- Step 1: Calls `get_community_summaries()` from the graph store, fetching all community summaries.\n",
    "- Step 2: For each community, it generates a specific answer to the input query using `generate_answer_from_summary()`.\n",
    "- Step 3: Consolidates all community-level answers into a final cohesive response via `aggregate_answers()`.\n",
    "\n",
    "This multi-stage flow ensures that the query is answered holistically, taking into account multiple knowledge clusters rather than depending on a single passage or isolated match.\n",
    "\n",
    "\n",
    "## 4.3. Answer Generation at the Community Level\n",
    "\n",
    "The `generate_answer_from_summary()` method:\n",
    "- Constructs a contextual system prompt embedding the specific community summary alongside the user’s query.\n",
    "- Sends this structured prompt to the LLM.\n",
    "- Cleans and extracts the LLM’s reply to remove any system artifacts (like assistant: prefixes).\n",
    "\n",
    "By grounding the LLM’s reasoning within a narrow topical scope (one community at a time), this method produces focused, contextually relevant partial answers.\n",
    "\n",
    "\n",
    "## 4.4. Final Answer Aggregation\n",
    "\n",
    "The `aggregate_answers()` method:\n",
    "- Takes all intermediate answers generated from different communities.\n",
    "- Forms a combined prompt asking the LLM to synthesize a final, concise answer from these fragments.\n",
    "- Again, sanitizes the output for a clean and polished result.\n",
    "\n",
    "This final aggregation step weaves together different knowledge perspectives into one cohesive narrative — ideal for complex, multi-topic queries.\n",
    "\n",
    "\n",
    "In traditional RAG systems, retrieval often stops at fetching similar passages.\n",
    "`GraphRAGQueryEngine` goes a step further:\n",
    "- It interprets structured summaries.\n",
    "- It reasons across communities.\n",
    "- It synthesizes multi-source answers with minimal hallucination risk.\n",
    "\n",
    "This leads to higher accuracy, better coherence, and superior explainability — all critical factors for production-grade GenAI applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6e20573-53be-48a7-b3ac-d3bbf85c99e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import CustomQueryEngine\n",
    "from llama_index.core.llms import LLM\n",
    "\n",
    "\n",
    "class GraphRAGQueryEngine(CustomQueryEngine):\n",
    "    graph_store: GraphRAGStore\n",
    "    llm: LLM\n",
    "\n",
    "    def custom_query(self, query_str: str) -> str:\n",
    "        \"\"\"Process all community summaries to generate answers to a specific query.\"\"\"\n",
    "        community_summaries = self.graph_store.get_community_summaries()\n",
    "        community_answers = [\n",
    "            self.generate_answer_from_summary(community_summary, query_str)\n",
    "            for _, community_summary in community_summaries.items()\n",
    "        ]\n",
    "\n",
    "        final_answer = self.aggregate_answers(community_answers)\n",
    "        return final_answer\n",
    "\n",
    "    def generate_answer_from_summary(self, community_summary, query):\n",
    "        \"\"\"Generate an answer from a community summary based on a given query using LLM.\"\"\"\n",
    "        prompt = (\n",
    "            f\"Given the community summary: {community_summary}, \"\n",
    "            f\"how would you answer the following query? Query: {query}\"\n",
    "        )\n",
    "        messages = [\n",
    "            ChatMessage(role=\"system\", content=prompt),\n",
    "            ChatMessage(\n",
    "                role=\"user\",\n",
    "                content=\"I need an answer based on the above information.\",\n",
    "            ),\n",
    "        ]\n",
    "        response = self.llm.chat(messages)\n",
    "        cleaned_response = re.sub(r\"^assistant:\\s*\", \"\", str(response)).strip()\n",
    "        return cleaned_response\n",
    "\n",
    "    def aggregate_answers(self, community_answers):\n",
    "        \"\"\"Aggregate individual community answers into a final, coherent response.\"\"\"\n",
    "        # intermediate_text = \" \".join(community_answers)\n",
    "        prompt = \"Combine the following intermediate answers into a final, concise response.\"\n",
    "        messages = [\n",
    "            ChatMessage(role=\"system\", content=prompt),\n",
    "            ChatMessage(\n",
    "                role=\"user\",\n",
    "                content=f\"Intermediate answers: {community_answers}\",\n",
    "            ),\n",
    "        ]\n",
    "        final_response = self.llm.chat(messages)\n",
    "        cleaned_final_response = re.sub(\n",
    "            r\"^assistant:\\s*\", \"\", str(final_response)\n",
    "        ).strip()\n",
    "        return cleaned_final_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fdf968-efa0-4305-ad17-c63510ebfb1c",
   "metadata": {},
   "source": [
    "# 5. Process the Input data\n",
    "\n",
    "Before building graphs or querying information, the first step is to prepare the raw text into a format that downstream AI models and pipelines can handle efficiently. The provided code walks through a simple but critical preprocessing pipeline that transforms a large text file into manageable, chunked units ready for further processing.\n",
    "\n",
    "## 5.1. Loading the Source Document\n",
    "- \"A Christmas Carol\" is a novell by Charles Dickens, published in 1843. We have downloaded it as `book.txt`. It is opened and its content is read into memory.\n",
    "- This raw text is wrapped into a `Document` object (from `LlamaIndex`’s core APIs), which standardizes it for consistent handling across different pipeline stages.\n",
    "\n",
    "## 5.2. Initializing the Sentence-Level Chunking Strategy\n",
    "- A `SentenceSplitter` is initialized to divide the large text into smaller, sentence-level chunks.\n",
    "- Each chunk is up to 1024 tokens long, with an overlap of 20 tokens between adjacent chunks.\n",
    "\n",
    "## 5.3. Splitting the Document into Nodes\n",
    "- The `SentenceSplitter` processes the document and produces a list of nodes.\n",
    "- Each node represents a chunk of text, optimized for feeding into downstream knowledge extraction models or graph builders.\n",
    "\n",
    "Chunking is a foundational step in any Retrieval-Augmented or Graph-based AI pipeline:\n",
    "- It balances granularity and context.\n",
    "- It ensures efficient LLM querying by controlling token size.\n",
    "- It preserves semantic flow by introducing overlaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcbde80e-584d-44c0-9e90-b41fc21907f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core import Document\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "with open(\"./book.txt\") as f:\n",
    "    doc = f.read() \n",
    "text = doc\n",
    "\n",
    "\n",
    "documents = [Document(text=text)]\n",
    "\n",
    "\n",
    "# Step 3: Initialize a chunking (node parser) strategy – sentence-level splitting\n",
    "\n",
    "splitter = SentenceSplitter(\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=20,\n",
    ")\n",
    "nodes = splitter.get_nodes_from_documents(documents)\n",
    "\n",
    "# # Step 5: Load nodes (chunks) into a list\n",
    "# chunked_docs = [node.text for node in nodes]\n",
    "nodes[0]\n",
    "len(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed0df9f-f976-4854-9715-a6b8af7c56ac",
   "metadata": {},
   "source": [
    "# 6. Define a Structured Extraction Prompt\n",
    "\n",
    "Provides the LLM with explicit instructions on extracting entities, relationships, and descriptions, ensuring structured, parsable JSON output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b7b7b8d-50e2-4cb5-b3a0-2645857becba",
   "metadata": {},
   "outputs": [],
   "source": [
    "KG_TRIPLET_EXTRACT_TMPL = \"\"\"\n",
    "-Goal-\n",
    "Given a text document, identify all entities and their entity types from the text and all relationships among the identified entities.\n",
    "Given the text, extract up to {max_knowledge_triplets} entity-relation triplets.\n",
    "\n",
    "-Steps-\n",
    "1. Identify all entities. For each identified entity, extract the following information:\n",
    "- entity_name: Name of the entity, capitalized\n",
    "- entity_type: Type of the entity\n",
    "- entity_description: Comprehensive description of the entity's attributes and activities\n",
    "\n",
    "2. From the entities identified in step 1, identify all pairs of (source_entity, target_entity) that are *clearly related* to each other.\n",
    "For each pair of related entities, extract the following information:\n",
    "- source_entity: name of the source entity, as identified in step 1\n",
    "- target_entity: name of the target entity, as identified in step 1\n",
    "- relation: relationship between source_entity and target_entity\n",
    "- relationship_description: explanation as to why you think the source entity and the target entity are related to each other\n",
    "\n",
    "3. Output Formatting:\n",
    "- Return the result in valid JSON format with two keys: 'entities' (list of entity objects) and 'relationships' (list of relationship objects).\n",
    "- Exclude any text outside the JSON structure (e.g., no explanations or comments).\n",
    "- If no entities or relationships are identified, return empty lists: { \"entities\": [], \"relationships\": [] }.\n",
    "\n",
    "-An Output Example-\n",
    "{\n",
    "  \"entities\": [\n",
    "    {\n",
    "      \"entity_name\": \"Albert Einstein\",\n",
    "      \"entity_type\": \"Person\",\n",
    "      \"entity_description\": \"Albert Einstein was a theoretical physicist who developed the theory of relativity and made significant contributions to physics.\"\n",
    "    },\n",
    "    {\n",
    "      \"entity_name\": \"Theory of Relativity\",\n",
    "      \"entity_type\": \"Scientific Theory\",\n",
    "      \"entity_description\": \"A scientific theory developed by Albert Einstein, describing the laws of physics in relation to observers in different frames of reference.\"\n",
    "    },\n",
    "    {\n",
    "      \"entity_name\": \"Nobel Prize in Physics\",\n",
    "      \"entity_type\": \"Award\",\n",
    "      \"entity_description\": \"A prestigious international award in the field of physics, awarded annually by the Royal Swedish Academy of Sciences.\"\n",
    "    }\n",
    "  ],\n",
    "  \"relationships\": [\n",
    "    {\n",
    "      \"source_entity\": \"Albert Einstein\",\n",
    "      \"target_entity\": \"Theory of Relativity\",\n",
    "      \"relation\": \"developed\",\n",
    "      \"relationship_description\": \"Albert Einstein is the developer of the theory of relativity.\"\n",
    "    },\n",
    "    {\n",
    "      \"source_entity\": \"Albert Einstein\",\n",
    "      \"target_entity\": \"Nobel Prize in Physics\",\n",
    "      \"relation\": \"won\",\n",
    "      \"relationship_description\": \"Albert Einstein won the Nobel Prize in Physics in 1921.\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "-Real Data-\n",
    "######################\n",
    "text: {text}\n",
    "######################\n",
    "output:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50c631e-289c-41bf-95fd-eafa4ded4abf",
   "metadata": {},
   "source": [
    "# 7. Entity and Relationship Extraction\n",
    "\n",
    "Initializes a custom extractor that sends each text chunk to the LLM, extracts a maximum of 2 triples, and parses the output into nodes and edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c9cc52f-d6bc-4b2b-aa7d-b8fa299e0a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def parse_fn(response_str: str) -> Any:\n",
    "    json_pattern = r\"\\{.*\\}\"\n",
    "    match = re.search(json_pattern, response_str, re.DOTALL)\n",
    "    entities = []\n",
    "    relationships = []\n",
    "    if not match:\n",
    "        return entities, relationships\n",
    "    json_str = match.group(0)\n",
    "    try:\n",
    "        data = json.loads(json_str)\n",
    "        entities = [\n",
    "            (\n",
    "                entity[\"entity_name\"],\n",
    "                entity[\"entity_type\"],\n",
    "                entity[\"entity_description\"],\n",
    "            )\n",
    "            for entity in data.get(\"entities\", [])\n",
    "        ]\n",
    "        relationships = [\n",
    "            (\n",
    "                relation[\"source_entity\"],\n",
    "                relation[\"target_entity\"],\n",
    "                relation[\"relation\"],\n",
    "                relation[\"relationship_description\"],\n",
    "            )\n",
    "            for relation in data.get(\"relationships\", [])\n",
    "        ]\n",
    "        return entities, relationships\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(\"Error parsing JSON:\", e)\n",
    "        return entities, relationships\n",
    "\n",
    "\n",
    "kg_extractor = GraphRAGExtractor(\n",
    "    llm=llm,\n",
    "    extract_prompt=KG_TRIPLET_EXTRACT_TMPL,\n",
    "    max_paths_per_chunk=2,\n",
    "    parse_fn=parse_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e43db6c-923e-44d7-bdfb-a5ab4e6dc37c",
   "metadata": {},
   "source": [
    "# 8. Build the Knowledge Graph\n",
    "\n",
    "- Uses `GraphRAGExtractor` to build a property graph.\n",
    "- Stores graph data inside a customized `GraphRAGStore`, which will later detect communities and generate summaries.\n",
    "\n",
    "Also lets take a look at few Entity, Relation and Relation descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "447beb5f-6350-4453-b7a3-d4cebf4e5323",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting paths from text: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 45/45 [00:46<00:00,  1.02s/it]\n",
      "Generating embeddings: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.99s/it]\n",
      "Generating embeddings: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  1.88it/s]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import PropertyGraphIndex\n",
    "\n",
    "index = PropertyGraphIndex(\n",
    "    nodes=nodes,\n",
    "    property_graph_store=GraphRAGStore(),\n",
    "    kg_extractors=[kg_extractor],\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3c2f993-4c4c-4968-8ffc-174fff9ea347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EntityNode(label='entity', embedding=None, properties={'triplet_source_id': 'a96a0fc3-9253-4e07-bccb-4f9433fc0e37'}, name='turkey')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(index.property_graph_store.graph.nodes.values())[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef63b8d9-d0e8-4daf-aa49-5d58821a0bbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Relation(label='former partner', source_id='Ebenezer Scrooge', target_id='Marley', properties={'relationship_description': 'Ebenezer Scrooge was a former partner of Marley in business.', 'triplet_source_id': 'e2249b72-2769-4475-a037-625fdec11e3b'})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(index.property_graph_store.graph.relations.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01971c7c-9397-4503-b437-378744ef215f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ebenezer Scrooge was a former partner of Marley in business.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(index.property_graph_store.graph.relations.values())[0].properties[\n",
    "    \"relationship_description\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82173c11-0b0e-473d-b3a9-9f74a84c8819",
   "metadata": {},
   "source": [
    "# 9. Community Detection and Summarization\n",
    "\n",
    "- Converts the graph into a NetworkX structure.\n",
    "- Applies Hierarchical Leiden clustering to group related entities.\n",
    "- Automatically generates LLM-based summaries for each community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca07b8f8-5644-4460-a8b0-9a5d31d3ffd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/llamaindex-graphrag-dev/lib/python3.11/site-packages/graspologic/partition/leiden.py:607: UserWarning: Leiden partitions do not contain all nodes from the input graph because input graph contained isolate nodes.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "index.property_graph_store.build_communities()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23b7745-eb8c-4fd1-9f22-72dbd045c276",
   "metadata": {},
   "source": [
    "# 10. Query Processing with GraphRAG\n",
    "\n",
    "Initializes a specialized query engine that uses community summaries to answer questions efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1603f0ec-173b-4ee8-b5fa-2ce001f1f526",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = GraphRAGQueryEngine(\n",
    "    graph_store=index.property_graph_store, llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a22f300-da17-4325-a3c4-389ffbe870c1",
   "metadata": {},
   "source": [
    "# 11. Lets ask few Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69ef45cd-a701-4954-a64e-7c54ad5e02e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################################\n",
      "\n",
      "QUESTION : What is the significance of Christmas Eve in A Christmas Carol?\n",
      "\n",
      "GRAPHRAG ANSWER : Christmas Eve holds significant importance in \"A Christmas Carol\" as it serves as the catalyst for the transformation of Ebenezer Scrooge. On this night, Scrooge is visited by the spirits of Marley, Christmas Past, Present, and Yet to Come, leading to his redemption and change of heart. The interactions on Christmas Eve highlight themes of redemption, reflection, and the power of human connection, ultimately shaping the narrative dynamics and character development in the story.\n",
      "\n",
      "##########################################\n",
      "\n",
      "QUESTION : How does the setting of Victorian London contribute to the story's themes?\n",
      "\n",
      "GRAPHRAG ANSWER : The setting of Victorian London in \"A Christmas Carol\" significantly contributes to the story's themes by emphasizing societal disparities, compassion, generosity, and the transformative power of redemption. Through the contrast between wealth and poverty, the city serves as a backdrop that underscores the importance of empathy, kindness, and community support. Additionally, the historical context of Victorian London enriches the narrative with themes of social responsibility, moral awakening, and the interconnectedness of individuals within a diverse and evolving society.\n",
      "\n",
      "##########################################\n",
      "\n",
      "QUESTION : Describe the chain of events that leads to Scrooge's transformation.\n",
      "\n",
      "GRAPHRAG ANSWER : Scrooge's transformation in \"A Christmas Carol\" is brought about by a series of events and encounters that challenge his beliefs and attitudes towards Christmas and life in general. It begins with the visitation of Marley's Ghost, his deceased business partner, who warns him of the consequences of his selfish and greedy ways. This encounter serves as a wake-up call for Scrooge, planting the seed for his redemption. Following Marley's visit, Scrooge is visited by the Ghosts of Christmas Past, Present, and Yet to Come, who guide him through a journey of self-discovery and reflection on his past, present, and potential future. Through these encounters and revelations, Scrooge comes to understand the impact of his actions on others and the true meaning of Christmas. Ultimately, this series of supernatural interventions and self-reflection leads to Scrooge's profound transformation from a miserly and cold-hearted individual to a compassionate and generous benefactor, embodying the spirit of Christmas and redemption.\n",
      "\n",
      "##########################################\n",
      "\n",
      "QUESTION : How does Dickens use the different spirits (Past, Present, and Future) to guide Scrooge?\n",
      "\n",
      "GRAPHRAG ANSWER : In \"A Christmas Carol,\" Charles Dickens uses the spirits of Past, Present, and Future to guide Scrooge on a transformative journey towards redemption and understanding the true meaning of Christmas. Through reflections on his past, interactions with those around him, and glimpses into his potential future, Scrooge learns the importance of human connection, empathy, and generosity, ultimately leading to his redemption and a newfound appreciation for the spirit of Christmas.\n",
      "\n",
      "##########################################\n",
      "\n",
      "QUESTION : Why does Dickens choose to divide the story into \"staves\" rather than chapters?\n",
      "\n",
      "GRAPHRAG ANSWER : Charles Dickens chose to divide the story into \"staves\" in \"A Christmas Carol\" to align with the musical theme of the novella, infusing the narrative with a rhythmic and harmonious structure that mirrors the themes of redemption and transformation central to the plot. This choice emphasizes the interconnectedness of characters and highlights the collaborative and harmonious relationships within the story, enhancing the overall thematic significance and creating a unique and engaging narrative flow.\n",
      "\n",
      "##########################################\n"
     ]
    }
   ],
   "source": [
    "queries = [\n",
    "    \"What is the significance of Christmas Eve in A Christmas Carol?\",\n",
    "    \"How does the setting of Victorian London contribute to the story's themes?\",\n",
    "    \"Describe the chain of events that leads to Scrooge's transformation.\",\n",
    "    \"How does Dickens use the different spirits (Past, Present, and Future) to guide Scrooge?\",\n",
    "    \"Why does Dickens choose to divide the story into \\\"staves\\\" rather than chapters?\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    response = query_engine.query(query)\n",
    "    print(\"##########################################\\n\")\n",
    "    print(f\"QUESTION : {query}\\n\")\n",
    "    print(f\"GRAPHRAG ANSWER : {response}\\n\")\n",
    "print(\"##########################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889f136f-0010-4e13-b891-1e3b38019c5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
